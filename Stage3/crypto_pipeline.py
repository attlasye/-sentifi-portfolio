# crypto_pipeline.py# FINAL VERSION with new technical indicators (RSI, MACD) and fixesfrom __future__ import annotationsimport argparseimport loggingimport timefrom pathlib import Pathfrom typing import List, Optional, Tuple # FIX: Added Tupleimport numpy as npimport pandas as pdimport requestsimport randomfrom sklearn.preprocessing import MinMaxScalerimport warnings                    warnings.filterwarnings(              action="ignore",    message="All-NaN slice encountered",    category=RuntimeWarning,)# ---------------------------------------------------------------------------# Logging --------------------------------------------------------------------logging.basicConfig(    level=logging.INFO,    format="%(asctime)s [%(levelname)s] %(message)s",    datefmt="%Y-%m-%d %H:%M:%S",)# ---------------------------------------------------------------------------# Directory helpers ----------------------------------------------------------def _ensure_dir(root: Path, sub: str | Path) -> Path:    """Return *root/sub* as Path, creating parents as needed."""    path = root / sub    path.mkdir(parents=True, exist_ok=True)    return pathdef build_crypto_dirs(base_dir: str | Path = ".") -> Path:    """Create organized crypto data directory"""    root = Path(base_dir).resolve()    data_dir = root / "results" / "crypto_data"    data_dir.mkdir(parents=True, exist_ok=True)    return data_dir# ---------------------------------------------------------------------------# API helpers ----------------------------------------------------------------BASE_URL = "https://data-api.coindesk.com"def _headers(api_key: str) -> dict[str, str]:    return {"authorization": f"Apikey {api_key}"}def get_top_coins(    api_key: str,    pages: List[int],    limit: int = 100,    sort_by: str = "CIRCULATING_MKT_CAP_USD",) -> List[str]:    """Return a list of coin symbols across *pages* sorted by *sort_by*."""    coins: List[str] = []    for page in pages:        url = (            f"{BASE_URL}/asset/v1/top/list?"            f"page={page}&page_size={limit}"            f"&sort_by={sort_by}&sort_direction=DESC"            "&groups=ID,BASIC,MKT_CAP"        )        resp = requests.get(url, headers=_headers(api_key), timeout=30)        data = resp.json()        if "Data" not in data or "LIST" not in data["Data"]:            logging.warning("Page %d returned no data: %s", page, data.get("Message"))            continue        for coin in data["Data"]["LIST"]:            coins.append(coin["SYMBOL"])        logging.info("Collected %d symbols from page %d", len(data["Data"]["LIST"]), page)    if not coins:        raise RuntimeError("No symbols retrieved. Check API key or parameters.")    return coinsdef get_daily_ohlcv(    symbol: str,    api_key: str,    limit: int = 2000,    currency: str = "USD",) -> pd.DataFrame | None:    """Return daily OHLCV for *symbol* or None on error."""    url = (        f"{BASE_URL}/index/cc/v1/historical/days"        f"?market=cadli&instrument={symbol}-{currency}"        f"&limit={limit}&aggregate=1&fill=true&apply_mapping=true"    )    resp = requests.get(url, headers=_headers(api_key), timeout=45)    data = resp.json()    if data.get("Response") == "Error" or "Data" not in data:        logging.warning("No data for %s: %s", symbol, data.get("Message"))        return None    df = pd.DataFrame(data["Data"])    df["date"] = pd.to_datetime(df["TIMESTAMP"], unit="s")    df = df.rename(        columns={            "OPEN": "open",            "HIGH": "high",            "LOW": "low",            "CLOSE": "close",            "VOLUME": "btc_volume",            "QUOTE_VOLUME": "usd_volume",        }    )    df = df[["date", "open", "high", "low", "close", "usd_volume", "btc_volume"]].copy()    df["usd_volume_mil"] = df["usd_volume"] / 1e6    df["symbol"] = symbol    df.set_index(["symbol", "date"], inplace=True)    return df# ----------------------------------------------------------------------# NEW Function# ----------------------------------------------------------------------def get_daily_ohlcv2(    symbol: str,    api_key: str,    limit: int = 2000,    currency: str = "USD",    max_retries: int = 3,    wait: float = 1.0,    verbose: bool = True,          ) -> Optional[pd.DataFrame]:    """    Download daily OHLCV for *symbol* from the Coindesk Crypto-Compare API.    Parameters    ----------    symbol        : str   – e.g. 'BTC', 'ETH'    api_key       : str   – your Coindesk/CC API key    limit         : int   – how many days to return (<= 2000)    currency      : str   – quote currency (default 'USD')    max_retries   : int   – times to retry if TIMESTAMP missing or error    wait          : float – seconds to wait between retries    Returns    -------    pd.DataFrame indexed by ['symbol', 'date'] or None if all retries fail.    """    url = (        f"{BASE_URL}/index/cc/v1/historical/days"        f"?market=cadli&instrument={symbol}-{currency}"        f"&limit={limit}&aggregate=1&fill=true&apply_mapping=true"    )    for attempt in range(1, max_retries + 1):        try:            safe_headers = {k: ('***' if k.lower() == 'authorization' else v)                            for k, v in _headers(api_key).items()}            logging.info("REQUEST -> GET %s | hdrs=%s", url, safe_headers)                        resp = requests.get(url, headers=_headers(api_key), timeout=30)            # ----------------------- VERBOSE DIAGNOSTICS --------------------            if verbose:                safe_headers = {k: ("***" if k.lower() == "authorization" else v)                                for k, v in resp.request.headers.items()}                logging.info(                    "[%s] HTTP %s  |  req-hdrs=%s  |  rate-remaining=%s",                    symbol,                    resp.status_code,                    safe_headers,                    resp.headers.get("x-ratelimit-remaining"),                )                logging.debug("[%s] raw-json=%s", symbol, resp.text[:500])            # ----------------------------------------------------------------            data = resp.json()            # API-level error or missing payload            if data.get("Response") == "Error" or "Data" not in data:                logging.warning(                    "No data for %s (attempt %d/%d): %s",                    symbol,                    attempt,                    max_retries,                    data.get("Message"),                )                raise ValueError("API response error")            # Make sure TIMESTAMP exists; otherwise force retry            if not data["Data"] or "TIMESTAMP" not in data["Data"][0]:                logging.warning(                    "TIMESTAMP missing for %s (attempt %d/%d) – retrying …",                    symbol,                    attempt,                    max_retries,                )                raise KeyError("TIMESTAMP")            # -----------------------------------------------------------------            # Normal parsing path            # -----------------------------------------------------------------            df = pd.DataFrame(data["Data"])            df["date"] = pd.to_datetime(df["TIMESTAMP"], unit="s")            df = df.rename(                columns={                    "OPEN": "open",                    "HIGH": "high",                    "LOW": "low",                    "CLOSE": "close",                    "VOLUME": "btc_volume",                    "QUOTE_VOLUME": "usd_volume",                }            )            df = df[                ["date", "open", "high", "low", "close", "usd_volume", "btc_volume"]            ].copy()            # Convenience: express USD volume in millions            df["usd_volume_mil"] = df["usd_volume"] / 1e6            df["symbol"] = symbol            df.set_index(["symbol", "date"], inplace=True)            return df        except (requests.RequestException, ValueError, KeyError) as exc:            # Connection problem OR explicit retry trigger            if attempt < max_retries:                time.sleep(wait)                continue            logging.error("Failed to fetch OHLCV for %s: %s", symbol, exc)    # All retries exhausted    return None# ----------------------------------------------------------------------# Stage 1 – ETL --------------------------------------------------------------def stage1_etl(    api_key: str,    pages: List[int],    top_limit: int = 100,    history_limit: int = 2000,    currency: str = "USD",    sleep_sec: float = 1,    data_dir: Path | None = None,    filename: str = "stage_1_crypto_data.csv",) -> pd.DataFrame:    """    Download OHLCV history for the top coins and return a tidy DataFrame.    """    logging.info("Fetching list of top coins …")    symbols = get_top_coins(api_key, pages, top_limit)    logging.info("Total symbols collected: %d", len(symbols))    all_frames: List[pd.DataFrame] = []    _download_count = 0               # tracks how many symbols processed        for sym in symbols:        logging.info("Downloading history for %s", sym)        df = get_daily_ohlcv2(sym, api_key, history_limit, currency)            if df is not None:            all_frames.append(df)            _download_count += 1            # base pause + small jitter        time.sleep(sleep_sec + random.uniform(0.0, 0.5))            # extra cool-off every 5 downloads        if _download_count % 5 == 0:            extra_wait = random.uniform(1.0, 2.0)            logging.info("Cooling off for %.2f s after %d downloads", extra_wait, _download_count)            time.sleep(extra_wait)    if not all_frames:        raise RuntimeError("No historical data retrieved.")    data = pd.concat(all_frames).sort_index()    if data_dir is not None:        out_path = data_dir / filename        data.to_csv(out_path)        logging.info("Stage 1 CSV written to %s", out_path)    return data# ---------------------------------------------------------------------------# Stage 2 – feature engineering ---------------------------------------------# FINAL VERSION WITH NEW TECHNICAL INDICATORS (RSI, MACD)def calculate_rsi(prices, period: int = 14) -> pd.Series:    delta = prices.diff()    gain = (delta.where(delta > 0, 0)).ewm(span=period, adjust=False).mean()    loss = (-delta.where(delta < 0, 0)).ewm(span=period, adjust=False).mean()    rs = gain / loss    return 100 - (100 / (1 + rs))def calculate_macd(prices) -> Tuple[pd.Series, pd.Series]:    exp1 = prices.ewm(span=12, adjust=False).mean()    exp2 = prices.ewm(span=26, adjust=False).mean()    macd = exp1 - exp2    signal = macd.ewm(span=9, adjust=False).mean()    return macd, macd - signaldef stage2_feature_engineering(    tidy_prices: pd.DataFrame | None = None,    csv_path: Path | None = None,    data_dir: Path | None = None,    filename: str = "stage_2_crypto_data.csv",) -> pd.DataFrame:    """    Create volume shocks, momentum, volatility, weekly returns, and    save the cleaned weekly data set.    """    if tidy_prices is None:        if csv_path is None:            raise ValueError("Provide either tidy_prices or csv_path.")        logging.info("Reading Stage 1 CSV from %s", csv_path)        tidy_prices = pd.read_csv(            csv_path, index_col=["symbol", "date"], parse_dates=["date"]        )    df = tidy_prices.reset_index().sort_values(["symbol", "date"]).copy()    df['date'] = pd.to_datetime(df['date'])        df['usd_volume'] = pd.to_numeric(df['usd_volume'], errors='coerce')    df.loc[df['usd_volume'] <= 0, 'usd_volume'] = np.nan    # Volume shocks ---------------------------------------------------------        for m in [7, 14, 21, 28, 42]:        rolling_mean = (            df.groupby("symbol")["usd_volume"]              .shift(1)              .rolling(window=m, min_periods=m)              .mean()        )        with np.errstate(divide="ignore", invalid="ignore"):            log_vol   = np.log(df["usd_volume"])            log_roll  = np.log(rolling_mean)        col = f"v_{m}d"        df[col] = log_vol - log_roll                  df[col] = df[col].replace([np.inf, -np.inf], np.nan)      # Log returns -----------------------------------------------------------    df["log_return"] = np.log1p(        df.groupby("symbol")["close"].pct_change()    )    df["log_return"] = np.where(df["log_return"] > 2, 2, df["log_return"])    df = df.replace([-np.inf, np.inf], np.nan)        # Momentum and volatility ----------------------------------------------    for m in [14, 21, 28, 42, 90]:        shifted = df.groupby("symbol")["log_return"].shift(7)        df[f"momentum_{m}"] = (            np.exp(                shifted.rolling(m, min_periods=m).sum()            )            - 1.0        )        df[f"volatility_{m}"] = (            df.groupby("symbol")["log_return"]            .rolling(m, min_periods=m)            .std()            .reset_index(level=0, drop=True)        ) * np.sqrt(365.0)    # Short-term reversal proxy --------------------------------------------    df["strev_daily"] = df["log_return"]        # Value-at-Risk ---------------------------------------------------------------    for m in [14, 21, 28]:        df[f"var_{m}"] = (            df.groupby("symbol")["log_return"]              .transform(                  lambda s: s.rolling(window=m, min_periods=m)                             .quantile(0.05, interpolation="lower")              )              .mul(-1)        )    # Amihud illiquidity ratio ---------------------------------------------    df["abs_ret"] = df["log_return"].abs()  *100    df["amihud_raw"] = df["abs_ret"] / df['usd_volume_mil']    for m in [14, 21, 28]:        df[f"amihud_{m}"] = (            df.groupby("symbol")["amihud_raw"]              .transform(lambda s: s.rolling(window=m, min_periods=m).mean())        )    df = df.drop(columns=["abs_ret", "amihud_raw"])    df = df.replace([-np.inf, np.inf], np.nan)        # --- ADD NEW TECHNICAL INDICATORS (FIXED) ---    logging.info("Calculating new technical indicators (RSI, MACD)...")    df["rsi"] = df.groupby("symbol")["close"].transform(calculate_rsi)        # Use a more robust approach for MACD that is Pandas-idiomatic    def calculate_macd_for_group(prices: pd.Series) -> pd.Series:        if len(prices) > 26:            macd, _ = calculate_macd(prices)            return macd        else:            return pd.Series(0, index=prices.index)    def calculate_macd_signal_for_group(prices: pd.Series) -> pd.Series:        if len(prices) > 26:            _, signal = calculate_macd(prices)            return signal        else:            return pd.Series(0, index=prices.index)                df["macd"] = df.groupby("symbol")["close"].transform(calculate_macd_for_group)    df["macd_signal"] = df.groupby("symbol")["close"].transform(calculate_macd_signal_for_group)        dfw = (        df.set_index("date")        .groupby("symbol")        .resample("D")        .last()        .droplevel("symbol")    )    dfw["return"] = dfw.groupby("symbol")["close"].pct_change()    dfw["return"] = np.where(dfw["return"] > 2, 2, dfw["return"])    dfw['strev_resamp'] = dfw["return"]    dfw = dfw.reset_index()        stable_tickers = [        "USD", "USDT", "USDC", "TUSD", "BUSD", "PAX", "USDP", "GUSD",        "DAI", "SUSD", "USDN", "FRAX", "USDX", "USDJ", "XUSD", "USDD",        "UST", "USTC",        "EUR", "EURT", "EURS", "EUROC", "SEUR", "SEUR", "SEUR", "SEUR",        "AEUR", "EURC", "AGEUR", "PAR","PAXG", "PYUSD", "USD1", "USDE"    ]    wrapped_tickers = [        "WBTC", "WETH", "WBNB", "WSTETH", "WUSDC", "WUSDT",        "WCRO", "WFTM", "WTRX", "WCELO", "WFIL", "WGLMR",        "WXRP", "WLTC", "WSOL", "WADA",    ]        tickers_to_drop = {t.upper() for t in stable_tickers + wrapped_tickers}    is_exact_drop   = dfw["symbol"].str.upper().isin(tickers_to_drop)    has_usd_substr  = dfw["symbol"].str.upper().str.contains("USD", na=False)    dfw = dfw[~(is_exact_drop | has_usd_substr)].copy()    dfw = dfw[dfw["return"] > -1.0]    dfw = dfw.replace([-np.inf, np.inf], np.nan)        col_order = [        "date", "symbol", "return", "open", "high", "low", "close","usd_volume",        "btc_volume", "v_7d",        "v_14d", "v_21d", "v_28d", "v_42d", "momentum_14",        "volatility_14", "momentum_21", "volatility_21", "momentum_28",        "volatility_28", "momentum_42", "volatility_42", "momentum_90",        "volatility_90", "strev_daily","strev_resamp",        'var_14', 'var_21', 'var_28', 'amihud_14', 'amihud_21', 'amihud_28',        'rsi', 'macd', 'macd_signal' # Add new columns    ]    dfw = dfw[[c for c in col_order if c in dfw.columns]].copy()        print("\nCross-sectional scaling...")    dfr = dfw.copy()    instruments = [c for c in col_order if c not in ["date", "symbol", "return"]]    dfr[instruments] = dfr[instruments].apply(pd.to_numeric, errors="coerce")    dfr = dfr.replace([-np.inf, np.inf], np.nan)    dfr = dfr.sort_values(["date", "symbol"])    dfr = dfr.reset_index(drop=True)    dfr['index'] = range(0,len(dfr))    def rank_scale_minus1_to1(series):        median_val = np.nanmedian(series)        series = series.fillna(median_val)        if series.nunique() <= 1:            return pd.Series(0.0, index=series.index)        ranks = series.rank().values.reshape(-1, 1)        scaler = MinMaxScaler(feature_range=(-1, 1))        scaled = scaler.fit_transform(ranks).flatten()        return pd.Series(scaled, index=series.index)    for sv in instruments:        new_col = f"{sv}_z"        print(f"Scaling {sv} by date cross-section into {new_col} ...")        dfr[new_col] = (            dfr.groupby("date")[sv]               .transform(rank_scale_minus1_to1)        )        if data_dir is not None:        out_path = data_dir / filename        dfr.to_csv(out_path, index=False)        logging.info("Stage 2 CSV written to %s", out_path)    return dfrdef _parse_args() -> argparse.Namespace:  # pragma: no cover    p = argparse.ArgumentParser(        prog="crypto_pipeline",        description="Crypto ETL (Stage 1) and feature engineering (Stage 2)",    )    p.add_argument("--api_key", required=True, help="CryptoCompare API key")    p.add_argument("--pages", type=int, nargs="+", default=[1], help="Pages for top coins")    p.add_argument("--top_limit", type=int, default=100, help="Coins per page")    p.add_argument("--history_limit", type=int, default=2000, help="Days of history")    p.add_argument("--currency", default="USD", help="Quote currency")    p.add_argument("--week", default="week_crypto", help="Output folder")    return p.parse_args()